---
title: "gDR suite"
author: "gDR team"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Running the drug response processing pipeline}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction {#intro}

Over decades, many departments across gRED and Roche have generated large amounts of drug response screening data using Genentech's rich drug compounds inventory. While extensive labor and time has been invested to generate these data, they are not analyzed in a standardized manner for meaningful comparison. On one hand, large screens are performed across many cell lines and drugs in a semi-automated manner. On the other hand, small-scale studies, which focused on factors that contribute to sensitivity and resistance to certain therapies, are generally performed by each individual scientist with limited automation. These are complementary approaches but were rarely handled the same way. Commercial softwares are available for analyzing large datasets, whereas researchers for small-scale datasets often process data ad hoc through software like PRISM.

Here, we propose a suite of computational tools that enable the processing, archiving, and visualization of drug response data from any experiment, regardless of size or experimental design, thus ensuring reproducibility and implementation of the Findable, Accessible, Interoperable, and Reusable (F.A.I.R.) principles. 

The gDR suite presents a full stack solution to systematically analyze and store drug response data. The core of gDR lies in its database which hosts thousands of experiments and datasets generated by different individual scientists. It includes large panel drug screens with simple experimental designs of single agent treatments, as well as more complex combinational treatments - either with a 2nd drug as a fixed dose or a matrix. When conditions are further complicated with co-treatment of ligands or with cell line genetic perturbations, e.g shRNA treatment or resistant cell lines obtained from a previous screen, as long as the conditions are well defined in metadata, these data will be compatible with the gDR model.

The processing pipeline takes as input metadata files which includes: metadata on the experimental design, template files specifying the drugs and cell lines used, and the raw data output files obtained from a plate reader or a scanner. Quality control, data normalization, and metric calculations are performed by the processing software and then the data is pushed and stored in the gDR database. The gDR suite provides an Rshiny application for the convenience of users to upload these different metadata files. Data processed by other commercial software or data residing from other databases can also be easily pushed into the gDR database through the REST API.

Once the data is stored in the database, there are multiple ways to visualize the data depending on the scientific needs. The primary method to do is through our RShiny visualization tool 'gDRviz'. Here, users can search and select experiments present in the database, and use downstream visualization modules to look at dose response curves, heatmaps, etc.

For now we share our initial gDR suite for loading, processing, and managing the data.

## R Packages {#rpackages}

gDR suite consists of a few packages that power our app and make it a comprehensive tool.
All the packages under the gDR umbrella are stored in the [gDR platform GitHub organization](https://github.roche.com/gdrplatform/).

We are happy to share with you our packages for importing, processing and managing gDR data:
* gDRimport
* gDRcore
* gDRutils
* gDRtestData

## Data structures 
The gDR data model is based on the SummarizedExperiment and BumpyMatrix. If readers are unfamiliar with these data models, we recommend first reading ____. The SummarizedExperiment data structure enables ease of subsetting within the SummarizedExperiment object, but also in conjunction with genomic data for the same samples which may be stored in a MultiAssayExperiment. The BumpyMatrix allows us to store multi-dimensional data while retaining a matrix abstraction.

This data structure in the core data structure that all downstream processing functions as well as visualization tools operate off of.

## Overview
The gDR suite was designed in a modular manner with the intention that as long as a user can jump into the pipeline at any stage as suits his or her needs. The full pipeline involves:
1. Aggregating all raw data and metadata into a single long table.
2. Transforming the long table into a SummarizedExperiment object with BumpyMatrix assays by specifying what columns belong on rows, columns, and nested.
3. Normalizing, averaging, and fitting data.


## Quick start

### Aggregating raw data and metadata (1)

The gDR suite ultimately requires a single, long merged table containing both raw data and metadata.

To support a common use case, we also provide a convenience function that takes three objects: manifest, template(s), and raw data to create this single, long merged table for the user.

Exemplary data can be found here:

```{r, echo=TRUE, results='asis'}
library(gDR)

# Define path for data stored in gDR package
dataDir <- system.file("extdata", "data1", package = "gDRimport")

# Extract path for example raw_data
manifest <- list.files(dataDir, pattern = "manifest", full.names = TRUE)
template <- list.files(dataDir, pattern = "Template", full.names = TRUE)
raw_data <- list.files(dataDir, pattern = "^RawData", full.names = TRUE)

manifest
template
raw_data
```

Using the convenience function `import_data`, the long table is easily created: 

```{r, echo=TRUE, results='asis', warning=FALSE, results='hide', message=FALSE}
# Import data
imported_data <- import_data(manifest, template, raw_data)
```

To use this function, the functions will expect certain "identifiers" that tell the processing functions which columns in the long table correspond to certain expected fields. For more details regarding these identifiers, see the "Details" section of `?identifiers`. Use `set_identifier` to set up the correct mappings between the expected fields and your long table column names.

### Transforming data into a SummarizedExperiment (2)
Next, we can transform the long table into our initial SummarizedExperiment object.
```
se <- create_SE(imported_data)
se
```
Note that this has created a SummarizedExperiment object with `rowData`, `colData`, `metadata` and 2 `assays`.


### Normalizing, averaging, and fitting data (3)
Next, we can normalize, average, and fit the data of interest. 
```
se <- normalize_SE(se)
se <- average_SE(se)
se <- fit_SE(se)
se
```

### runDrugResponseProcessingPipeline
Steps (2) and (3) can be combined into a single step through a convenience function: `runDrugResponseProcessingPipeline`.

```{r, echo=TRUE, results='asis', warning=FALSE, results='hide', message=FALSE}
# Run gDR pipeline
se <- runDrugResponseProcessingPipeline(imported_data)
```
```{r, echo=TRUE}
se
```

The processing pipeline will create a SummarizedExperiment containing raw treated and control assays and then will normalize the data based on the mapped treated conditions and their corresponding controls. The normalized data will then be averaged, and drug-response metrics will be calculated from the averaged data.

SummarizedExperiment object is made up of multiple assays, such as:

```{r, echo=TRUE}
SummarizedExperiment::assayNames(se)
```

that can be easily transformed to `data.table` format using `convert_se_assay_to_dt` function:

```{r, echo=TRUE, eval=FALSE}
convert_se_assay_to_dt(se, "Metrics")
```

```{r, echo=FALSE}
library(kableExtra)
kbl(convert_se_assay_to_dt(se, "Metrics")) %>%
  kable_paper() %>%
  scroll_box(width = "700px", height = "300px")
```

